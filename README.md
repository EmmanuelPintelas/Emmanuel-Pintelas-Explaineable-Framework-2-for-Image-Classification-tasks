# EmmanuilXXX-Explaineable-Framework-2-for-Image-Classification-tasks
An explainable/interpretable machine learning model is able to make reasoning about its predictions in understable terms to humans, while its prediction mechanism/function is totally transparent and interpretable. These properties are essential in order to trust model’s predictions since humans demand and need by nature some sort of explanation for any decision making, especially when these decisions affect critical aspects such as health, rights, security, and educational issues. Image classification is an area in machine learning and computer vision in which deep convolutional neural networks have mainly flourished because of their high performance score. These models are considered black boxes suffering in terms of transparency, interpretability, and explainability, and thus in trust. Nevertheless, explainability in image classification problems is by default a very complicated and challenging task. In fact, it is so difficult that even humans cannot explain their own decisions on such problems. In this work it is proposed a novel explainable image classification framework applying it on skin cancer and plant diseases prediction problems. This framework aims to combine segmentation and clustering techniques aiming to extract texture features from various sub-regions of the input image. Then a feature filtering and cleaning procedure is applied on these extracted features in order to ensure that the proposed model will be also reliable and trustful, while these final extracted features are utilized for training an intrinsic lineal white box prediction model. Finally, a hierarchy-based tree approach was created, in order to provide a meaningful interpretation of the model’s decision behavior.
